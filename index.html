<!doctype html>
<html>

<head>
<title>Lue Fan</title>

<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Lue Fan, NLPR, Institute of Automation, Chinese Academy of Sciences">
<meta name="description" content="Lue Fan's home page">
<link rel="stylesheet" href="template.css" type="text/css" />
<link rel="icon" href="images/photo.jpg" type="image/jpeg" />

<!-- Show more content -->
<script type="text/javascript">
    function toggle_vis(id) {
        var e = document.getElementsByClassName(id);
        var showText = document.getElementById("showText");
        for (var i = 0; i < e.length; i++) {
            if (e[i].style.display == "none") {
                e[i].style.display = "inline";
                showText.innerHTML = "[Show less]";
            } else {
                e[i].style.display = "none";
                showText.innerHTML = "[Show more]";
            }
        }
    }

    // Image preview functionality
    function showImagePreview(imgSrc) {
        var preview = document.getElementById('imagePreview');
        var previewImg = document.getElementById('previewImg');
        previewImg.src = imgSrc;
        preview.style.display = 'flex';
    }

    function hideImagePreview() {
        var preview = document.getElementById('imagePreview');
        preview.style.display = 'none';
    }

    // Initialize image preview functionality when page loads
    document.addEventListener('DOMContentLoaded', function() {
        var publicationImages = document.querySelectorAll('.publication_image img');
        publicationImages.forEach(function(img) {
            img.addEventListener('click', function() {
                showImagePreview(this.src);
            });
        });
    });
</script>

</head>

<body>

<!-- Image Preview Modal -->
<div id="imagePreview" class="image-preview" onclick="hideImagePreview()">
    <img id="previewImg" src="" alt="Preview">
</div>

<div id="layout-content" style="margin-top:25px">

<table>
	<tbody>
		<tr>
			<td width="75%">
				<div id="toptitle">
					<h1>Lue Fan (范略)<h1>
				</div>

                <h3 class="title">Assistant Professor</h3>

				<p class="contact-info">
                    <span class="label">Affiliation</span><br/>
                    NLPR (模式识别实验室), Institute of Automation<br/>
                    Chinese Academy of Sciences<br/><br/>
                    <span class="label">Email</span><br/>
                    lue.fan at ia.ac.cn
				</p>

                <div class="social-links">
                    <a href="https://scholar.google.com/citations?user=6ZzmkHEAAAAJ&hl=zh-CN&oi=ao" class="social-link">
                        <svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 24a7 7 0 1 1 0-14 7 7 0 0 1 0 14zm0-24L0 9.5l4.838 3.94A8 8 0 0 1 12 9a8 8 0 0 1 7.162 4.44L24 9.5z"/></svg>
                        Google Scholar
                    </a>
                    <a href="https://github.com/Abyssaledge" class="social-link">
                        <svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.37 0 0 5.37 0 12c0 5.31 3.435 9.795 8.205 11.385.6.105.825-.255.825-.57 0-.285-.015-1.23-.015-2.235-3.015.555-3.795-.735-4.035-1.41-.135-.345-.72-1.41-1.23-1.695-.42-.225-1.02-.78-.015-.795.945-.015 1.62.87 1.845 1.23 1.08 1.815 2.805 1.305 3.495.99.105-.78.42-1.305.765-1.605-2.67-.3-5.46-1.335-5.46-5.925 0-1.305.465-2.385 1.23-3.225-.12-.3-.54-1.53.12-3.18 0 0 1.005-.315 3.3 1.23.96-.27 1.98-.405 3-.405s2.04.135 3 .405c2.295-1.56 3.3-1.23 3.3-1.23.66 1.65.24 2.88.12 3.18.765.84 1.23 1.905 1.23 3.225 0 4.605-2.805 5.625-5.475 5.925.435.375.81 1.095.81 2.22 0 1.605-.015 2.895-.015 3.3 0 .315.225.69.825.57A12.02 12.02 0 0 0 24 12c0-6.63-5.37-12-12-12z"/></svg>
                        GitHub
                    </a>
                </div>

			</td>
			<td width="25%">
				<img src="images/photo.jpg" width="100%" class="profile-photo"/>
			</td>
		<tr>
	</tbody>
</table>

<h2>About Me</h2> 

<div style="display: flex">
    <p>
        I am currently an assistant professor in NLPR, Institute of Automation, Chinese Academy of Sciences, working with Prof. <a href="https://zhaoxiangzhang.net/chinese/">Zhaoxiang Zhang</a>. I got my Ph.D. degree from this lab in June 2024, supervised by Prof. <a href="https://zhaoxiangzhang.net/chinese/">Zhaoxiang Zhang</a>, and bachelor's degree from Xi'an Jiaotong University (XJTU) in 2019, majoring in automation. I was a research intern at TuSimple, supervised by Dr. <a href="https://scholar.google.com/citations?user=yAWtq6QAAAAJ&hl=en">Naiyan Wang</a> and Dr. <a href="https://happynear.wang/">Feng Wang</a>. Currently, I am working closely with Prof. <a href="https://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a> @ MMLab.
    </p>
</div>

<!-- <div style="display: flex; margin-bottom: -10px">
    <p>
        My research interests focus on the <strong>perception/decision/simulation</strong> algorithms in autonomous driving scenarios. During Ph.D., my representative research lies in a series of algorithms for <strong>LiDAR-based fully sparse detection</strong>, supporting the super long-range perception and enhancing the driving safety. 
    </p>
</div> -->

<div class="recruitment-box">
    <p>
        <strong>Our NLPR lab is actively recruiting interns and postdocs!</strong><br/>
        If you are interested in <strong>Autonomous Driving / Embodied AI / Coding Agent</strong>, please feel free to reach out via email: {lue.fan, zhaoxiang.zhang}@ia.ac.cn
    </p>
</div>

<h2>News</h2>

<ul>
    <li>
        <div class="marker"><span class="news-date">2026-01</span> Three papers accepted to <strong>ICLR 2026</strong> and one paper accepted to <strong>ICRA 2026</strong></div>
    </li>
    <li class="news-award">
        <div class="marker"><span class="news-date">2025-11</span> Received the <strong>Outstanding Doctoral Dissertation Award of CSIG</strong> (中国图象图形学学会博士学位论文激励计划)</div>
    </li>
    <li class="news-award">
        <div class="marker"><span class="news-date">2025-08</span> Received the <strong>Outstanding Doctoral Dissertation Award of CAS</strong> (中科院优秀博士学位论文)</div>
    </li>
    <li>
        <div class="marker"><span class="news-date">2025-07</span> Three papers accepted to <strong>ICCV 2025</strong></div>
    </li>
    <li>
        <div class="marker"><span class="news-date">2025-02</span> FreeSim and FlexDrive accepted by <strong>CVPR 2025</strong></div>
    </li>
    <li>
        <div class="marker"><span class="news-date">2025-01</span> LAW accepted by <strong>ICLR 2025</strong></div>
    </li>
</ul>

<div class="show_button">
    <a href="javascript:toggle_vis('news')" id="showText">[Show more]</a>
</div>

<!-- <h2>Research Summary</h2>

<p>
My research spans three key areas in autonomous driving: <strong>perception</strong>, <strong>decision-making</strong>, and <strong>simulation</strong>. I have developed algorithms for LiDAR-based fully sparse 3D object detection, enabling super long-range perception capabilities for autonomous driving systems.
</p>

<p>
Recently, I focus on driving simulation technologies, including novel view synthesis and world models for autonomous driving systems. My work aims to bridge the gap between simulation and real-world autonomous driving scenarios.
</p> -->

<h2>Selected Work</h2>

<p>*: Equal Contribution; †: Corresponding Author</p>

<h3>2026</h3>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/neoverse_teaser.png">
    </div>
    <div class="publication_title">
        NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos<span class="hot-tag">Hot</span><br/>
        Yuxue Yang, <strong>Lue Fan†</strong>(project lead), Ziqi Shi, Junran Peng, Feng Wang, Zhaoxiang Zhang†<br/>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2601.00393" class="paper-link">Paper</a>
            <a href="https://neoverse-4d.github.io/" class="paper-link">Project</a>
            <a href="https://mp.weixin.qq.com/s/SebDjFNQ2124szYkOTETDg" class="paper-link">新智元</a>
        </div>
    </div>
</div>

<h3>2025</h3>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/drivevla_teaser.png">
    </div>
    <div class="publication_title">
        DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving<br/>
        Yingyan Li*, Shuyao Shang*, Weisong Liu*, Bing Zhan*, Haochen Wang*, Yuqi Wang, Yuntao Chen, Xiaoman Wang, Yasong An, Chufeng Tang, Lu Hou, <strong>Lue Fan†</strong>, Zhaoxiang Zhang†<br/>
        <span class="venue-badge venue-iclr">ICLR 2026</span><br/>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2510.12796" class="paper-link">Paper</a>
            <a href="https://github.com/BraveGroup/DriveVLA-W0" class="paper-link">Code</a>
        </div>
    </div>
</div>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/embodiedcoder_teaser.png">
    </div>
    <div class="publication_title">
        EmbodiedCoder: Parameterized Embodied Mobile Manipulation via Modern Coding Model<br/>
        Zefu Lin, Rongxu Cui, Chen Hanning, Xiangyu Wang, Junjia Xu, Xiaojuan Jin, Chen Wenbo, Hui Zhou, <strong>Lue Fan†</strong>, Wenling Li, Zhaoxiang Zhang†<br/>
        <span class="venue-badge venue-icra">ICRA 2026</span><br/>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2510.06207" class="paper-link">Paper</a>
            <a href="https://embodiedcoder.github.io/EmbodiedCoder/" class="paper-link">Project</a>
        </div>
    </div>
</div>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/wote.png">
    </div>
    <div class="publication_title">
        End-to-End Driving with Online Trajectory Evaluation via BEV World Model<br/>
        Yingyan Li, Yuqi Wang, Yang Liu, Jiawei He, <strong>Lue Fan†</strong>, Zhaoxiang Zhang†<br/>
        <span class="venue-badge venue-iccv">ICCV 2025</span><br/>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2504.01941" class="paper-link">Paper</a>
            <a href="https://github.com/liyingyanUCAS/WoTE" class="paper-link">Code</a>
        </div>
    </div>
</div>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/layeranimate.png">
    </div>
    <div class="publication_title">
        LayerAnimate: Layer-level Control for Animation<br/>
        Yuxue Yang, <strong>Lue Fan</strong>, Zuzeng Lin, Feng Wang, Zhaoxiang Zhang<br/>
        <span class="venue-badge venue-iccv">ICCV 2025</span><br/>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2501.08295" class="paper-link">Paper</a>
            <a href="https://layeranimate.github.io" class="paper-link">Project</a>
        </div>
    </div>
</div>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/freesim_teaser.png">
    </div>
    <div class="publication_title">
        FreeSim: Toward Free-viewpoint Camera Simulation in Driving Scenes<br/>
        <strong>Lue Fan*</strong>, Hao Zhang*, Qitai Wang, Hongsheng Li†, Zhaoxiang Zhang†<br/>
        <span class="venue-badge venue-cvpr">CVPR 2025</span><br/>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2412.03566" class="paper-link">Paper</a>
            <a href="https://drive-sim.github.io/freesim" class="paper-link">Project</a>
        </div>
    </div>
</div>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/flexdrive.png">
    </div>
    <div class="publication_title">
        FlexDrive: Toward Trajectory Flexibility in Driving Scene Reconstruction and Rendering<br/>
        Jingqiu Zhou*, <strong>Lue Fan*</strong>, Linjiang Huang, Xiaoyu Shi, Si Liu, Z. Zhang†, Hongsheng Li†<br/>
        <span class="venue-badge venue-cvpr">CVPR 2025</span><br/>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2502.21093" class="paper-link">Paper</a>
        </div>
    </div>
</div>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/freevs_teaser.png">
    </div>
    <div class="publication_title">
        FreeVS: Generative View Synthesis on Free Driving Trajectory<br/>
        Qitai Wang, <strong>Lue Fan</strong>, Yuqi Wang, Yuntao Chen†, Zhaoxiang Zhang†<br/>
        <span class="venue-badge venue-iclr">ICLR 2025</span><br/>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2410.18079" class="paper-link">Paper</a>
            <a href="https://freevs24.github.io/" class="paper-link">Project</a>
        </div>
    </div>
</div>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/law_teaser.png">
    </div>
    <div class="publication_title">
        Enhancing End-to-End Autonomous Driving with Latent World Model<br/>
        Yingyan Li, <strong>Lue Fan</strong>, Jiawei He, Yuqi Wang, Yuntao Chen, Zhaoxiang Zhang, Tieniu Tan<br/>
        <span class="venue-badge venue-iclr">ICLR 2025</span><br/>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2406.08481" class="paper-link">Paper</a>
            <a href="https://github.com/BraveGroup/LAW" class="paper-link">Code</a>
        </div>
    </div>
</div>

<h3>2024</h3>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/fsdv2_teaser.png">
    </div>
    <div class="publication_title">
        FSD V2: Improving Fully Sparse 3D Object Detection with Virtual Voxels<br/>
        <strong>Lue Fan</strong>, Feng Wang, Naiyan Wang, Zhaoxiang Zhang<br/>
        <span class="venue-badge venue-tpami">TPAMI 2024</span><br/>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2308.03755" class="paper-link">Paper</a>
            <a href="https://github.com/TuSimple/SST" class="paper-link">Code</a>
        </div>
    </div>
</div>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/opensatmap_teaser.png">
    </div>
    <div class="publication_title">
        OpenSatMap: A Fine-grained High-resolution Satellite Dataset for Large-scale Map Construction<br/>
        Hongbo Zhao, <strong>Lue Fan</strong>, Yuntao Chen, Haochen Wang, Yuran Yang, Xiaojuan Jin, Yixin Zhang, Gaofeng Meng, Zhaoxiang Zhang<br/>
        <span class="venue-badge venue-neurips">NeurIPS 2024</span> D&amp;B Track<br/>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2410.23278" class="paper-link">Paper</a>
            <a href="https://opensatmap.github.io/" class="paper-link">Project</a>
        </div>
    </div>
</div>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/trimgs_teaser.png">
    </div>
    <div class="publication_title">
        Trim 3D Gaussian Splatting for Accurate Geometry Representation<br/>
        <strong>Lue Fan*</strong>, Yuxue Yang*, Minxing Li, Hongsheng Li†, Zhaoxiang Zhang†<br/>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2406.07499" class="paper-link">Paper</a>
            <a href="https://github.com/YuxueYang1204/TrimGS" class="paper-link">Code</a>
            <a href="https://trimgs.github.io/" class="paper-link">Project</a>
        </div>
    </div>
</div>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/voxelmamba_teaser.png">
    </div>
    <div class="publication_title">
        Voxel Mamba: Group-Free State Space Models for Point Cloud based 3D Object Detection<br/>
        Guowen Zhang, <strong>Lue Fan</strong>, Chenhang He, Zhen Lei, Zhaoxiang Zhang, Lei Zhang<br/>
        <span class="venue-badge venue-neurips">NeurIPS 2024</span><br/>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2406.10700" class="paper-link">Paper</a>
            <a href="https://github.com/gwenzhang/Voxel-Mamba" class="paper-link">Code</a>
        </div>
    </div>
</div>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/wm_teaser.png">
    </div>
    <div class="publication_title">
        Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving<br/>
        Yuqi Wang*, Jiawei He*, <strong>Lue Fan*</strong>, Hongxin Li*, Yuntao Chen†, Zhaoxiang Zhang†<br/>
        <span class="venue-badge venue-cvpr">CVPR 2024</span><br/>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2311.17918" class="paper-link">Paper</a>
            <a href="https://github.com/BraveGroup/Drive-WM" class="paper-link">Code</a>
            <a href="https://drive-wm.github.io" class="paper-link">Project</a>
        </div>
    </div>
</div>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/mixsup_teaser.png">
    </div>
    <div class="publication_title">
        MixSup: Mixed-grained Supervision for Label-efficient LiDAR-based 3D Object Detection<br/>
        Yuxue Yang, <strong>Lue Fan†</strong>, Zhaoxiang Zhang†<br/>
        <span class="venue-badge venue-iclr">ICLR 2024</span><br/>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2401.16305" class="paper-link">Paper</a>
            <a href="https://github.com/BraveGroup/PointSAM-for-MixSup" class="paper-link">Code</a>
        </div>
    </div>
</div>

<h3>2023</h3>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/ctrl_teaser.png">
    </div>
    <div class="publication_title">
        Once Detected, Never Lost: Surpassing Human Performance in Offline LiDAR based 3D Object Detection (CTRL)<br/>
        <strong>Lue Fan</strong>, Yuxue Yang, Yiming Mao, Feng Wang, Yuntao Chen, Naiyan Wang, Zhaoxiang Zhang<br/>
        <span class="venue-badge venue-iccv">ICCV 2023</span><span class="oral-tag">Oral</span><br/>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2304.12315v1" class="paper-link">Paper</a>
            <a href="https://github.com/TuSimple/SST" class="paper-link">Code</a>
        </div>
    </div>
</div>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/fsdpp_teaser.png">
    </div>
    <div class="publication_title">
        Super Sparse 3D Object Detection (FSD++)<br/>
        <strong>Lue Fan</strong>, Yuxue Yang, Feng Wang, Naiyan Wang, Zhaoxiang Zhang<br/>
        <span class="venue-badge venue-tpami">TPAMI 2023</span><br/>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2301.02562" class="paper-link">Paper</a>
            <a href="https://github.com/TuSimple/SST" class="paper-link">Code</a>
        </div>
    </div>
</div>

<h3>2022</h3>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/fsd_teaser.png">
    </div>
    <div class="publication_title">
        Fully Sparse 3D Object Detection (FSD)<br/>
        <strong>Lue Fan</strong>, Feng Wang, Naiyan Wang, Zhaoxiang Zhang<br/>
        <span class="venue-badge venue-neurips">NeurIPS 2022</span><br/>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2207.10035" class="paper-link">Paper</a>
            <a href="https://github.com/TuSimple/SST" class="paper-link">Code</a>
        </div>
    </div>
</div>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/sst_teaser.png">
    </div>
    <div class="publication_title">
        Embracing Single Stride 3D Object Detector with Sparse Transformer (SST)<br/>
        <strong>Lue Fan</strong>, Ziqi Pang, Tianyuan Zhang, Yu-Xiong Wang, Hang Zhao, Feng Wang, Naiyan Wang, Zhaoxiang Zhang<br/>
        <span class="venue-badge venue-cvpr">CVPR 2022</span><br/>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2112.06375" class="paper-link">Paper</a>
            <a href="https://github.com/TuSimple/SST" class="paper-link">Code</a>
        </div>
    </div>
</div>

<h3>2021</h3>

<div class="publication_container">
    <div class="publication_image">
        <img src="images/rangedet_teaser.png">
    </div>
    <div class="publication_title">
        RangeDet: In Defense of Range View for Lidar-based 3D Object Detection<br/>
        <strong>Lue Fan*</strong>, Xuan Xiong*, Feng Wang, Naiyan Wang, Zhaoxiang Zhang<br/>
        <span class="venue-badge venue-iccv">ICCV 2021</span><br/>
        <div class="paper-links">
            <a href="https://arxiv.org/abs/2103.10039" class="paper-link">Paper</a>
            <a href="https://github.com/TuSimple/RangeDet" class="paper-link">Code</a>
        </div>
    </div>
</div>

<div id="footer">
<div id="footer-text">
<!-- <script type="text/javascript" src="https://www.free-counters.org/count/frj4"></script><br> -->
<em>This website was automatically generated from personal information using Claude Code.</em>

<!-- 小型配色切换器 -->
<div style="margin-top: 15px; display: flex; align-items: center; justify-content: center; gap: 8px;">
    <span style="font-size: 12px; color: #888;">Theme:</span>
    <button onclick="setTheme('#155e75')" style="background:#155e75;width:20px;height:20px;border:2px solid #ccc;border-radius:4px;cursor:pointer;" title="墨湖蓝"></button>
    <button onclick="setTheme('#0e7490')" style="background:#0e7490;width:20px;height:20px;border:2px solid #ccc;border-radius:4px;cursor:pointer;" title="深湖蓝"></button>
    <button onclick="setTheme('#0891b2')" style="background:#0891b2;width:20px;height:20px;border:2px solid #ccc;border-radius:4px;cursor:pointer;" title="湖蓝"></button>
    <button onclick="setTheme('#2b6cb0')" style="background:#2b6cb0;width:20px;height:20px;border:2px solid #ccc;border-radius:4px;cursor:pointer;" title="经典蓝"></button>
    <button onclick="setTheme('#2c5282')" style="background:#2c5282;width:20px;height:20px;border:2px solid #ccc;border-radius:4px;cursor:pointer;" title="学院蓝"></button>
</div>

<script>
function setTheme(color) {
    document.documentElement.style.setProperty('--primary-color', color);
    document.documentElement.style.setProperty('--primary-light', color);
    document.documentElement.style.setProperty('--primary-dark', color);
    document.documentElement.style.setProperty('--accent-color', color);
}
</script>

</div>
</div>

</div>

</body>
</html>